{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition:\n",
    "    Steps:\n",
    "        1. Organize the data in an appropriate file structure (shown below).\n",
    "        2. Use transfer learning to train a pretrained model (vgg16) to predict cats or dogs.\n",
    "        3. Generate a submission file for Kaggle.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. organize data:\n",
    "    Make sure the data is structured in the following format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image_recognition_fs.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample is just a smaller subset of the original data, this is made so that the code can be tested quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import shutil\n",
    "import os, random\n",
    "import numpy as np\n",
    "# vgg contains the class to use Vgg16 model\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "# utils contains helper methods such as plotting images\n",
    "import utils; reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign data location\n",
    "path = '/home/ubuntu/courses/data/'\n",
    "# we can use the second path variable if we want to test our code on a smaler subset\n",
    "#path = '/home/ubuntu/courses/data/sample/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use transfer learning on a pretrained model (vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # limited by memory capabilities\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path = path+'train/', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path = path+'valid/', batch_size = batch_size)\n",
    "# fine tune the last layer of vgg16 model to give 2 probabilities instead of 1000 classes( vgg16 default output is 1000 probabilities)\n",
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 600s - loss: 0.1125 - acc: 0.9707 - val_loss: 0.0739 - val_acc: 0.9835\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 602s - loss: 0.0954 - acc: 0.9787 - val_loss: 0.0601 - val_acc: 0.9845\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "vgg.fit(batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate prediction on test images\n",
    "batches, preds = vgg.test(path+'test/', batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kaggle requests submission in a specific format\n",
    "# must contain a header id,label\n",
    "# every line should have image id,probability_of_dog\n",
    "filenames = batches.filenames\n",
    "# get the test file id's from filenames list\n",
    "file_ids = [f.replace('images/', '').replace('.jpg', '') for f in filenames]\n",
    "isDog = preds[:,1]\n",
    "# clip to prevent hig log loss, since this is the metric kaggle uses to measure the results\n",
    "isDog = isDog.clip(min=0.05, max=0.95)\n",
    "subm = np.stack([file_ids, isDog], axis = 1)\n",
    "subm = subm.astype('float')\n",
    "submission_file_name = 'submission_1.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach was able to score in the top 30% of the kaggle competition.\n",
    "To improve: \n",
    "    1. more epocs\n",
    "    2. try resnet, inception, other architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
